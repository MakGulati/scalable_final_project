{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport pickle\nimport time\nimport os\nimport os.path\nimport sys\nimport pandas as pd\nfrom torchsummary import summary\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 300\nBATCH_SIZE = 64\nLEARNING_RATE = 0.001\nNUM_CLASSES = 5\nTRAIN_DATA_PATH=\"../input/mayank-cheetah/output/\" #replace with directory containing output folder ccontaining datatset\nTRANSFORM_IMG = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n#     to normalize between(0.5,0.5)\n#     transforms.Normalize(mean=[.5, .5, .5],std=[1 ,1, 1]) \n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)\ntrain_data.__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, val_set = torch.utils.data.random_split(train_data, [int(np.floor(0.9*train_data.__len__())),int(train_data.__len__()-np.floor(0.9*train_data.__len__()))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = data.DataLoader(train_set, batch_size= BATCH_SIZE, shuffle=True, num_workers=4)\nval_set = data.DataLoader(val_set, batch_size= BATCH_SIZE, num_workers=4)\nval_set.__len__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #replace with directory containing output folder ccontaining datatset\nclasses = [d for d in os.listdir('../input/mayank-cheetah/output/') if os.path.isdir(os.path.join('../input/mayank-cheetah/output/', d))]\nclasses.sort()\nclass_to_idx = {classes[i]: i for i in range(len(classes))}\nclass_to_idx ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRIED THIS BASIC MODEL BUT ACCUARY WAS AROUND 66% SO NOT SUITABLE\n\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 24, kernel_size=9, stride=1, padding=0),\n            nn.BatchNorm2d(24),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(24, 48, kernel_size=7, stride=1, padding=0),\n            nn.BatchNorm2d(48),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(48, 96, kernel_size=5, stride=1, padding=0),\n            nn.BatchNorm2d(96),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(96, 192, kernel_size=3, stride=1, padding=0),\n            nn.BatchNorm2d(192),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc1 = nn.Sequential(\n            nn.Linear(10*10*192, 512),\n            nn.ReLU())\n        self.fc2 = nn.Sequential(\n            nn.Linear(512,128),\n            nn.ReLU())\n        self.fc3 = nn.Linear(128, num_classes)\n        self.sm = nn.Softmax(dim=-1)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        out = self.sm(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_model = ConvNet().to(device)\n#to print model summary\nsummary(first_model, input_size=(3, 224, 224))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Vgg model for tansfer learning\ncustom_model=torchvision.models.vgg11(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in custom_model.parameters(): #freezing gradients\n    param.requires_grad = False   \n\n#adding custom layers in the end for specfic tasks    \ncustom_model.classifier[-1] = nn.Sequential(\n               nn.Linear(4096, 512),\n               nn.ReLU(inplace=True),\n               nn.Linear(512, 128),\n               nn.ReLU(inplace=True),\n               nn.Linear(128, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_model = custom_model.to(device)\n#to print model summary\nsummary(custom_model, input_size=(3, 224, 224))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(custom_model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train the model\nlosses=[]\ntotal_step = len(train_set)\n#start time\n# torch.cuda.synchronize()\nsince = int(round(time.time()*1000))\nfor epoch in range(EPOCHS):\n    running_loss=0.0\n    for i, (images, labels) in enumerate(train_set):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = custom_model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n        \n        if (i+1) % 1 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, EPOCHS, i+1, total_step, loss.item()))\n    epoch_loss = running_loss / len(train_set)\n    losses.append(epoch_loss)\n#stop time\n# torch.cuda.synchronize()\ntime_elapsed = int(round(time.time()*1000)) - since\nprint ('training time elapsed {}ms'.format(time_elapsed)) #to print whole training time\n\n#saving losses in pickle to plot graph later\npickle_out = open(\"dict.pickle\",\"wb\")\npickle.dump(losses, pickle_out)\npickle_out.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for loading weights from previous saved checkpoints\ncustom_model.load_state_dict(torch.load('../input/checkpoints-dict/model.ckpt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model\ncustom_model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\npredict_list=[]\ntrue_list=[]\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in val_set:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = custom_model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        predict_list.append(predicted)\n        true_list.append(labels)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 2304 test images: {} %'.format(100 * correct / total))\n\n# Save the model checkpoint\n# torch.save(custom_model.state_dict(), 'model.ckpt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting training graph\n\nwith open('../input/checkpoints-dict/dict.pickle', 'rb') as f:\n# with open('dict.pickle', 'rb') as f:\n    x = pickle.load(f)\nplt.title('Training Loss')\nplt.xlabel('no. of epochs')\nplt.ylabel('epoch loss')\nplt.plot(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is to verify on particular image if classifies well\nfrom PIL import Image\n# from torch.autograd import Variable\nfrom IPython.display import display\nloader = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n\ndef image_loader(image_name):\n    \"\"\"load image, returns cuda tensor\"\"\"\n    image = Image.open(image_name).convert(\"RGB\")\n    image = loader(image).float()\n    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n    return image.cuda()  #assumes that you're using GPU\ndef image_show(image_name):\n    image = Image.open(image_name).convert(\"RGB\")\n    return(image)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base =\"../input/mayank-cheetah/output/\"\nimg_list=[\"concrete_cement/7a27cc58.png\",\"healthy_metal/7a22b3da.png\",\"incomplete/7a2aafae.png\",\"irregular_metal/7a4164c4.png\",\"other/7a3ea4aa.png\"]\n\nfor i in img_list:\n    image = image_loader(base+i)\n    display(image_show(base+i))\n    \n    #verification using one image at a time\n    _, predikt= torch.max(custom_model(image).data, 1)\n    idx_to_classes={0:'concrete_cement',\n      1:'healthy_metal',\n      2:'incomplete',\n      3:'irregular_metal',\n      4: 'other',\n      }\n    print(idx_to_classes[predikt.item()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = predict_list[0].cpu().numpy()\nfor i in range(1,val_set.__len__()):\n    a = np.hstack((a, predict_list[i].cpu().numpy()))\nb = true_list[0].cpu().numpy()\nfor i in range(1,val_set.__len__()):\n    b = np.hstack((b, true_list[i].cpu().numpy()))\n\ny_actu = pd.Series(b, name='Actual')\ny_pred = pd.Series(a, name='Predicted')\ndf_confusion = pd.crosstab(y_actu, y_pred, margins=True)\ndf_confusion_new = df_confusion.rename(columns=idx_to_classes, index=idx_to_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_confusion_new ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"torch_planet","language":"python","name":"torch_planet"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}